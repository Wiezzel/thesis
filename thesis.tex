\documentclass{pracamgr}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[backend=biber,style=numeric]{biblatex}
\usepackage{bold-extra}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{changepage}
\usepackage{chngcntr}
\usepackage{enumitem}
\usepackage{fixltx2e}
\usepackage{float}
\usepackage[T1]{fontenc} 
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[lighttt]{lmodern}
\usepackage{lstlinebgrd}
\usepackage{ltxtable}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{pbox}
\usepackage{pgf}
\usepackage{pgffor}
\usepackage{polski}
\usepackage{soul}
\usepackage{tikz}
\usepackage{xcolor}

\usetikzlibrary{arrows}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{intersections}
\usetikzlibrary{patterns}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes}

\author{Adam Wierzbicki}
\nralbumu{306441}
\title{Using Code History for Defect Prediction}
\tytulpl{Wykorzystanie historii kodu do predykcji błedów}
\kierunek{Informatyka}
\opiekun{
	prof. dr. hab. Krzysztofa Stencla\\
	Instytut Informatyki
}
\date{Marzec 2015}
\dziedzina{11.3 Informatyka}
\klasyfikacja{
	D. Software\\
	D.2. Software Engineering\\
	D.2.8 Metrics\\
	D.2.9 Management
}
\keywords{code history, defect, bug-proneness, prediction, repository, metrics, machine learning}

\bibliography{bibliography}

\renewcommand{\contentsname}{Table of contents}
\renewcommand{\listtablename}{List of tables}
\renewcommand{\listfigurename}{List of figures}
\renewcommand{\chaptername}{Chapter}
\renewcommand{\tablename}{Table}
\renewcommand{\figurename}{Figure}
\renewcommand{\lstlistingname}{Snippet}

\counterwithout{table}{chapter}
\counterwithout{figure}{chapter}
\lstset{numberbychapter=false}
\captionsetup[table]{labelfont=bf}
\captionsetup[figure]{labelfont=bf}
\captionsetup[lstlisting]{labelfont=bf}

\newcommand*{\noaddvspace}{\renewcommand*{\addvspace}[1]{}}
\addtocontents{lot}{\protect\noaddvspace}
\addtocontents{lof}{\protect\noaddvspace}

\newcolumntype{C}{>{\centering\arraybackslash}X}
\newcolumntype{D}[1]{>{\hsize=#1\hsize\centering\arraybackslash}X}
\renewcommand{\arraystretch}{1.5}

\newcommand{\cpbox}[2]{\pbox{#1}{\relax\ifvmode\centering\fi #2}}
\newcommand{\adjbl}[1]{\raisebox{0pt}[\height][0pt]{#1}}
\newcommand{\bpar}[1]{\medskip\noindent\textbf{#1}}
\newcommand{\bparsc}[1]{\medskip\noindent\textbf{\textsc{#1}}}
\newcommand{\bpartt}[1]{\medskip\noindent\textbf{\texttt{#1}}}

\captionsetup{belowskip=12pt,aboveskip=8pt}

\newenvironment{definition}[1]%
	{
		\medskip
		\begin{adjustwidth}{\parindent}{}
		\rlap{\textbf{#1}}\par\nobreak
	}
	{
		\end{adjustwidth}
	}
	
\lstnewenvironment{lstdiff}[1]
	{\lstset{multicols=2,tabsize=4,escapechar=|,language=Java,basicstyle=\ttfamily,linebackgroundcolor={\btLstHL{#1}}}}
	{}
	
\newenvironment{diff}[1]
	{
		\hspace*{-\parindent}
		\begin{minipage}{\textwidth}
		\captionsetup{width=\linewidth}
		\captionof{lstlisting}{#1}
	}
	{
		\end{minipage}
	}

\input{utils/highlight.tex}
\input{utils/rectangle.tex}

\begin{document}

\maketitle

\begin{abstract}
Detection of software defects has become one of the major challenges in the field of automated software engineering. Numerous studies have revealed that mining data from repositories could provide a substantial basis for defect prediction. In this thesis I introduce my approach towards this problem relying on the analysis of source code history and machine learning algorithms. I describe in detail the proposed computational procedures and explain their underlying assumptions. Following the theoretical basis, I  present the results of performed experiments which serve as an empirical assessment of the effectiveness of my methods.
\end{abstract}

\tableofcontents

\begingroup
  \let\cleardoublepage\relax
  \listoftables
  \listoffigures
\endgroup

\chapter{Introduction}
\label{cha:introduction}
Ever since the legendary first bug in 1947\footnote{According to \cite{first_bug} on 9th of September, 1947 an investigation of malfunctioning Harvard University Mark II Aiken Relay Calculator revealed a moth trapped between the points of relay \#70 in panel F. This event was reported in a log with the following statement: \textit{"First actual case of bug being found."}}, the detection of software faults has been a crucial part of the quality assurance process. Undiscovered bugs were the cause of many misfortunes with the crash of the \$500-million space rocket Ariane 5 being the most spectacular example.\footnote{On 4th of June, 1996 the rocket was launched by the European Space Agency from Kourou in French Guyana. It exploded only about 40 seconds after take-off due an error in the inertial reference system -- conversion of a 64-bit float to a 16-bit signed integer failed, because the number was larger than the largest storable value. \cite{ariane}} This work aims to contribute to the general improvement of software quality by investigating certain methods of error identification.

\section{Topic choice rationale}
\label{sec:topic_choice}
Various procedures have been applied to track down software defects before they would cause any problems. Most widely used techniques are testing and code reviewing. Both of them are quite successful, but unfortunately also very arduous. In order to improve the performance of these methods and reduce the programmers' effort needed to conduct them, automatic debugging programs are developed.

Such programs include a broad range of approaches towards detecting bugs. Some of them are run-time debuggers which help programmers analyse the execution of a program (either standalone as GDB \cite{gdb} or integrated with IDEs as \textsc{Microsoft Visual Studio Debugger} \cite{vs_debugger}). Others generate test cases using randomization and symbolic execution \cite{symbolic, puzzle}. Others support the reviewing process by highlighting potentially dangerous program parts.

Identification of such fault-prone elements could be performed using many different methods. There are tools which rely on hard-coded patterns of so-called ``bad code smells" (\textit{e.g.} \textsc{FindBugs} \cite{findbugs}). Other ones incorporate statistical analysis and machine learning, using various software metrics and properties (\textit{e.g.} HATARI \cite{hatari}). In this work I focus on the latter approach, because I find it more interesting and less frequently encountered. I decided to use historical metrics\footnote{Software metrics based on code change history. For details see Section \ref{sec:historical}.} which have proven to be well-suited for this task \cite{merits, comparative, how_and_why}. With my research I hope to extend the knowledge about methods of bug prediction, which is not only of theoretical but also of practical value. This thesis makes the following contributions:
\begin{itemize}
	\item Proposal of a novel method for bug prediction,
	\item Prototype implementation of the proposed method,
	\item Experimental evaluation of the implemented tool.
\end{itemize}

\section{Document structure}
\label{sec:structure}

\textbf{Chapter \ref{cha:introduction}} includes general introductory information for the thesis, topic choice rationale, and a sketch of the document structure. 

\bpar{Chapter \ref{cha:overview}} contains a broad and comprehensive description of the problem of bug prediction. I try to define in a possibly precise manner all terms relevant to this topic. Then I present the general goals of the prediction process and its consecutive phases. I describe several kinds of sub-problems which have to be solved in order to successfully develop an error detection instrument.

\bpar{Chapter \ref{cha:approach}} presents my approach towards the problem. The proposed method is based on extraction of historical information from a version control system and application of a machine learning algorithm. I describe the general model of a changing method\footnote{In the object-oriented-programming sense}, which is the principal element of the whole procedure, as well as successive phases such as feature extraction, identification of bug-fixes, assignment of bug-proneness scores and training a regressor. In the last section of this chapter, I list four different machine learning algorithms which are evaluated in Chapter \ref{cha:experiments}.

\bpar{Chapter \ref{cha:implementation}} describes the \textsc{ChangeAnalyzer} tool -- an implementation of the approach proposed in Chapter \ref{cha:approach}. I present all used technologies (languages and libraries), software architecture and the interface.

\bpar{Chapter \ref{cha:experiments}} contains a description of experiments which were performed to evaluate the proposed approach and compare the performance of different algorithms listed in Section \ref{sec:machine_learning}. I explain the methodology of these experiments and software configuration necessary to conduct them. Then I give an overview of the experimental data and finally present the results in form of tables and plots.

\bpar{Chapter \ref{cha:conclusions}} includes a commentary of the experimental results and possible threats to the validity of my research. After these, I try to fairly asses in general my approach towards the problem of bug prediction. In the last section I propose possible ways of continuation of my work on this topic.

\chapter{Problem overview}
\label{cha:overview}
Simply speaking, bug prediction is a procedure aimed at automatic detection of faults in an unreviewed and untested code, basing on reports about previously discovered bugs. However, this deceptively plain definition hides a very convoluted and problematical process. In this chapter I would like to explore its meanders and formulate a more precise description by answering questions such as: \emph{\textbf{What is it} exactly that we are predicting?} (Section \ref{sec:dependent_variable}) \emph{What is \textbf{the scope} of our prediction?} (Section \ref{sec:unit_of_analysis}) \emph{How do we learn about \textbf{former defects}?} (Section \ref{sec:identification}) and \emph{What \textbf{kinds of information} can we utilize in our prediction?} (Section \ref{sec:observable_variables})

\section{Terminology}
\label{sec:terminology}
Before referring the topic, I would like to define some concepts of major importance, namely: \emph{version control system}, \emph{software repository}, \emph{commit}, \emph{error}, \emph{bug-fix}, \emph{bug-tracker}, \emph{software metric}, and \emph{error model}. Below are given brief explications of how I understand these terms.

\medskip
\begin{definition}{Version control system}
``A system capable of recording the changes made to a file or a set of files over a time period in such a way that it allows us to get back in time from the future to recall a specific version of that file" \cite[p. 8]{git_book}. Usually, such system is used to manage the source code of a software project. Examples of VCSs are: \textsc{Apache}\textsuperscript{TM} \textsc{Subversion}\textsuperscript{\textregistered} \cite{subversion}, \textsc{Git} \cite{git}, or \textsc{Mercurial} \cite{mercurial}.
\end{definition}

\begin{definition}{Software repository}
A directory containing the source code of a software project, which is managed by a version control system. It could be either local or remote (accessed via HTTPS or SSH protocol).
\end{definition}

\begin{definition}{Commit}
``An atomic collection of changes to files in a repository. It contains all recorded local modifications that lead to a new revision of the repository" \cite{mercurial_wiki}. Apart from file changes, a commit contains meta-information such as: committer's name and e-mail address, date and time, and a description of changes. Commit is also often called \emph{changeset}.
\end{definition}

\begin{definition}{Error}
``An incorrect step, process, or data definition. For example, an incorrect instruction in a computer program" \cite[p. 31]{glossary}. Error is also called \emph{bug}, \emph{fault}, or \textit{defect}.
\end{definition}

\begin{definition}{Bug-fix}
A commit created in order to repair a software defect. It is usually distinguished by an appropriate commit message describing the fixed error.
\end{definition}

\begin{definition}{Bug tracking system}
A system that ``stores and manages all bug status and information such as the module where a bug occurred, when a bug was found, the severity of a bug, comments describing a bug’s effect on the software, instructions on replicating a bug, who reported the bug, and whether the bug has been fixed yet" \cite{adaptive}. Also called simply \emph{bug-tracker}. Examples of bug-trackers are: \textsc{Bugzilla} \cite{bugzilla} and \textsc{Jira} \cite{jira}.
\end{definition}

\begin{definition}{Software metric\footnote{I prefer the term ``measure" to be used in this context instead of ``metric", because it is more consistent with the mathematical understanding of measure and metric. However, ``software metric" has become a widely used phrase in software engineering, so I will incline to this tendency.}}
``A quantitative measure of the degree to which a system, component, or process possesses a given attribute" \cite[p. 47-48]{glossary}.
\end{definition}

\begin{definition}{Error model}
``In software evaluation, a model used to estimate or predict the number of remaining faults, required test time, and similar characteristics of a system" \cite[p. 31]{glossary}.
\end{definition}

\section{Dependent variable}
\label{sec:dependent_variable}
Prediction in general may be described as discovering a relation between several observable variables and a dependent variable (which may also be called \textit{target variable}). The discovered relation allows for estimating values of the target variable by analysing the observables. Trying to conform \textit{defect prediction} to this description seems problematic, because ``defect" does not name any well-defined variable. Therefore, the first fundamental concern of bug prediction is specifying a variable which will both suit our theoretical framework and embrace the common-sense intuitions.

The most obvious candidate (used \textit{e.g.} by Janes \textit{et al.} in \cite{Janes}) is the number of defects per software unit\footnote{\cite[p. 31]{glossary} endorses the choice of this kind of variables by defining error prediction as ``a quantitative statement about \textbf{the expected number} or nature of faults in a system or component" (emphasis added).} (for possible units see Section \ref{sec:unit_of_analysis}). However, as in many cases the numbers of bugs in software components are relatively small, a dichotomous boolean variable (indicating bug presence or absence) is used more often (\textit{e.g.} by Moser \textit{et al.} in \cite{comparative} or Giger \textit{et al.} in \cite{method-level}). More complex examples include \emph{fault density} -- number of faults divided by a code size measure (Tomaszewski \textit{et al.} \cite{Tomaszewski}), or expected number of repairs in some period of time (Hassan~\cite{complexity}).

As Arisholm \textit{et al.} reasonably state in \cite[p. 5]{systematic}, the choice of dependent variable should be determined by the further usage of the developed model. For assigning a software component for a detailed examination by a reviewer, a boolean variable is probably sufficient. On the other hand, creating a ranking or visualisation of the software quality requires numeric values, so number of defects or fault density would suit these purposes much better.

\section{Unit of analysis}
\label{sec:unit_of_analysis}
All of the variables defined in the above section are relative to some code segment. Size of such segment can span from a single function up to a whole project -- every possibility has its advantages and drawbacks. As nine of ten most popular programming languages are object-oriented \cite{popularity1,popularity2}, also the most commonly encountered units of analysis are related to OOP paradigm: module/package, class, or method/function.

Package-level prediction (used \textit{e.g.} by Jin \textit{et al.} in \cite{Jin}) can make use of structural software properties which cannot be computed for smaller units. However, the typical size of a package in some languages would make the corresponding fault-proneness value of very little value for practical application. Therefore, module- or package-level prediction is usually restricted to languages to which the class level does not apply, \textit{e.g.} Fortran, Pascal or Ada.

Class level (used \textit{e.g.} by Tomaszewski \textit{et al.} in \cite{Tomaszewski}) is the most popular choice among defect prediction studies. Probably, the reason for this popularity is the fact that class is the most natural and accustomed part of object-oriented software. In case of non-object-oriented languages, the middle-sized unit of analysis is file, which is easily separable and trackable with a VCS machinery. Class shares the advantages of file, since numerous software projects obey, either by virtue of conventional guidelines or due to language constraints, the \emph{one class per file} rule. The scope of a class or a file is also usually narrow enough to serve as an object of manual inspection in case of high bug-proneness score.

Using method or function as the unit of analysis is a more complicated task, but it has been also successfully accomplished (\textit{e.g.} by Giger \textit{et al.} in \cite{method-level}). Contrary to higher-level units described above, which can be identified using solely lists of files, ascribing faults to certain methods requires examining detailed information about code changes. I consider the results of method-level prediction most useful for debugging due to their fine granularity.

The scope of prediction is limited not only \emph{spatially} (by which I understand focusing on particular software components, as described above in this section), but also \emph{temporally}. One may predict the number (or presence) of bugs in a software release, over some fixed period of time, or in a single commit. This choice should also be affected by the later utilisation of predicted values.

\section{Identification of bugs}
\label{sec:identification}
Prediction of new bugs has to be based on historical data about the heretofore detected ones. However, identification of these is not a trivial task. It has become one of the major challenges in the discipline of \emph{mining software repositories}.

\subsection{Bug-fixes}
\label{sec:bug-fixes}
First step towards identification of bugs is the recognition of bug-fixing commits. Traditionally used heuristic (\textit{e.g.} by Kim in \cite{adaptive}) involves searching for keywords such as ``bug", ``issue" or ``fix" in commit messages. A more refined approach (used in \cite{Ekanayake, method-level, adaptive}), instead of relying on hard-coded patterns, utilizes information obtained from a bug-tracking system. Bug-trackers assign IDs to the tracked issues, which could be later referenced in change logs. As the vast majority of contemporary software projects uses bug-trackers, searching for these IDs in commit messages could therefore constitute a reliable method for detecting bug-fixes.

\subsection{Noise in error data}
\label{sec:noise}
Unfortunately, research has shown that the aforementioned methods are not quite successful. Herzig \textit{et al.} \cite{Herzig}, after analysing over 7,000 error reports, make the following statements:
\begin{itemize}
	\item ``More than 40\% of issue reports are inaccurately classified",
	\item ``33.8\% of all bug reports do not refer to corrective code maintenance",
	\item ``Due to misclassifications, 39\% of files marked as defective actually have never had a bug".
\end{itemize}
Bachmann \textit{et. al} in \cite{Bachmann} confirm the poor quality of error reporting (particularly in \textsc{Apache} project), by declaring that ``only 47.6\% of bug fix related commits are documented in the bug tracking database". Human failure and negligence makes the error data obtained from repositories highly biased an untrustworthy. Which in turn negatively affects bug localization and prediction \cites{Herzig, dealing, Kochnar}.

In order to improve the efficiency of fault detection, Wu \textit{et al.} have developed \textsc{ReLink} -- an automatic link\footnote{The term ``link" refers here to the links between bug reports and code changes.} recovery algorithm \cite{ReLink}. It uses machine learning approach with features such as:
\begin{itemize}
	\item The interval between the bug-fixing time and the change-commit time,
	\item Mapping between bug owners and change committers,
	\item The similarity between bug reports and change logs.
\end{itemize}
Evaluated on five open-source projects, \textsc{ReLink} achieved up to 26.6\% more recall than the traditional heuristics.

\subsection{Bug-introducing changes}
\label{sec:bug-introducing}
The next stage, after identifying bug-fixes, is linking them with \emph{bug-introducing changes}. No earlier than this is achieved, one can give the precise temporal location of an error. Several algorithms have been developed for this purpose. Probably the most popular one (used \textit{e.g.} by Kim \cite{adaptive} and Fukushima \textit{et al.} \cite{Fukushima}) is the SZZ algorithm developed by Śliwerski, Zimmermann \& Zeller in \cite{SZZ}. It relies on annotation graphs produced by CVS\footnote{\textsc{Concurrent Versions System} is an open-source version control system \cite{CVS}.} \emph{annotate}\footnote{The \emph{annotate} command allows to ``print the head revision of the trunk, together with information on the last modification for each line" \cite{CVS_annotate}. Its counterpart in \textsc{Git} and SVN is \emph{blame}.} command. This approach was criticised by in \cite{SZZ_revisited} by Williams \& Spacco, who propose a different technique using line-number maps and \textsc{DiffJ} -- a Java syntax-aware diff tool. Application of such algorithms allows to conclude with high certainty, which parts of software at which point of time are to be considered fault-prone.

\section{Observable variables}
\label{sec:observable_variables}
Prediction of the target variable has to be done through a measurement of some observable variables. A multitude of software metrics has been applied for this purpose. In this section I will present a condensed survey of the most common ones (and some less common, but interesting).

\subsection{Static metrics}
\label{sec:static}
Static metrics are ``measures of structural properties derived from the source code" \cite[p.~5]{systematic}. Their essential property (responsible for the name) is that they can be computed using only a snapshot (revision) of the code. Some of static metrics are specific to object-oriented software, other are more universal (\textit{e.g.} LOC or complexity). Examples of static metrics are listed in Table \ref{tab:static_metrics}. Please note, that metrics presented as low-level could be also used on higher levels by aggregation operations (\textit{e.g.} minimum, maximum or average).

\LTXtable{\textwidth}{tables/static_metrics.tex}

\subsection{Historical metrics}
\label{sec:historical}
As the name suggests, historical metric are obtained through the analysis of software history, which is stored in a version control system. Some of them are computed from raw change data (that is a series of code revisions), while others rely on meta-information stored by VCS such as commit times, authors, and descriptions. Historical metrics are also called \emph{process metrics} or \emph{change metrics}\footnote{However, in \cite{Fukushima} this last name is used for metrics which can be attributed to a \textit{change itself}, not to a software component.}. Table \ref{tab:historical_metrics} contains examples of such metrics.

\LTXtable{\textwidth}{tables/historical_metrics.tex}

\subsection{Micro-interaction metrics}
\label{sec:micro-interaction}
Micro-interaction metrics, proposed by Lee \textit{et al.} in \cite{micro_interaction}, are tracking behavioural interaction patterns of software developers. Usage of such metrics is motivated by research showing correlations between work habits and productivity \cite{LaToza}. They can be obtained from systems tracking developers' activities, such as \textsc{Mylyn} \cite{mylyn}, a task management plug-in for \textsc{Eclipse IDE}. Experimental results presented in \cite{micro_interaction} show that micro-interaction metrics outperform both static and historical metrics. However, the scope of their usage is very limited, due to requirement for programmers' activities registration, which is not so common in software projects. Examples of micro-interaction metrics are presented in Table \ref{tab:micro_interaction_metrics}.

\LTXtable{\textwidth}{tables/micro_interaction_metrics.tex}

\chapter{The proposed approach}
\label{cha:approach}
Having referred the general goals and issues connected with defect prediction, I would like to present my own approach towards this problem. I decided to perform the prediction on method level, which is the finest granularity reached by state-of-the-art algorithms. Similarly to \cite{method-level}, I chose to use historical metrics based on fine-grained source code changes. However, instead of relying on sparse software releases as time scope boundaries, I developed a model allowing for a more continuous prediction.

Underlying assumptions and a general outline of the model is presented in Section \ref{sec:model}. Then, in Section \ref{sec:features}, a detailed summary of the model's observable variables is given. Section \ref{sec:bug-proneness}, on the other hand, describes the hidden (target) variable. Section \ref{sec:machine_learning} explains four different machine learning algorithms which can be applied to the model.

\section{The model}
\label{sec:model}

\subsection{General assumptions}
\label{sec:general_assumptions}
Design of the further described model relies on the following fundamental assumptions:
\begin{enumerate}[label=(A\arabic*)]
	\item Every method, in every moment can be assigned a certain \textbf{\emph{bug-proneness}} level ranging between 0.0 and 1.0 (these values are arbitrary and do not denote any units).
	\item \textbf{Immediately before a bug-fix}, the method to be fixed is \textbf{undoubtedly deficient}, therefore its bug-proneness equals 1.0.
	\item \textbf{Immediately after a bug-fix}, the fixed method is \textbf{perfectly correct}, therefore its bug-proneness equals 0.0. 
\end{enumerate}
The above presumptions may be criticized in many ways as not accurately reflecting the reality. However, they were not intended to do so, but rather to form an idealization which would allow for a possibly simple modelling of the quality of software methods.

The bug-proneness parameter constitutes the target variable in my approach. As it is not precisely defined, one could use different methods of measuring this value. These are described later, in Section \ref{sec:bug-proneness}. An example of how bug-proneness score can change with succeeding commits is presented in Figure \ref{fig:bug_proneness}.

\begin{figure}[h]
\centering
\input{figures/bug_proneness.tex}
\caption{Bug-proneness alteration}
\label{fig:bug_proneness}
\end{figure}

\subsection{Instance construction}
\label{sec:instance_construction}

Since my aim is to follow changes affecting the analysed method, model instances are constructed from succeeding commits. However, as the mentioned changes are \emph{cumulative}, each instance corresponds to a whole \emph{chunk} of commits, rather than to single one. In virtue of assumptions A2 and A3, the maximal time scope of an instance is bounded by two adjacent bug-fixes. This means that instances range in size from single commits to continuous groups delimited by bug-fixes. Example of instance construction is presented in Figure \ref{fig:instance_construction}.

\begin{figure}[h]
\centering
\input{figures/instance_construction.tex}
\caption{Instance construction}
\label{fig:instance_construction}
\end{figure}

\section{Features}
\label{sec:features}
In order to be useful for prediction of the target variable (bug-proneness), the model incorporates several observable variables. I decided to use historical metrics (described in Section \ref{sec:historical}), which are considered superior to static measures (Section \ref{sec:static}) by many researchers \cite{merits, comparative, how_and_why} and at the same time are more widely applicable than micro-interaction metrics (Section \ref{sec:micro-interaction}).

Opposed to most works using historical metrics (\textit{e.g.} \cite{systematic, merits, micro_interaction, comparative, how_and_why}), which rely on standard source code differencing utilities\footnote{Such as \texttt{diff} and \texttt{diff3} provided by \textsc{GNU diffutils} package \cite{diffutils}.}, I employ a syntax-aware \emph{change distilling} algorithm developed by Fluri \textit{et al.} in \cite{change_distilling}, described in more detail in the next section. A similar approach was taken by Giger \textit{et al.} in \cite{method-level}.

\subsection{Change distilling}
\label{sec:change_distilling}
Change distilling algorithm, presented in \cite{change_distilling} aims to provide a smart, syntax-aware method of differencing source code files. It extracts \emph{fine-grained source code changes} (down to the level of single statements) by matching the \emph{abstract syntax trees}\footnote{Abstract syntax tree (AST) is the ``hierarchical syntactic structure" of a sentence belonging to some formal language (\textit{e.g.} a programming language). ``Every internal node of a parse tree is labeled with a nonterminal symbol; every leaf is labeled with a terminal symbol. Every subtree of a parse tree describes one instance of an abstraction in the sentence." \cite[p. 121]{concepts} An abstract syntax tree is also called a \emph{parse tree}, for it is generated by an act of \emph{parsing}.} representing the compared files.

Leaves of the ASTs are matched using the \emph{bi-gram Dice coefficient} -- an adaptation of an ecological association measure (proposed by Dice in \cite{Dice}) to the text distance measurement. The similarity of inner nodes is computed as a combination of node values similarities and subtrees similarities. After matching the appropriate nodes, the algorithm calculates the minimal number of \emph{insert}, \emph{delete}, \emph{move}, and \emph{update} operations required to transform the first version of the AST into the other. These elemental operations constitute fine-grained source code changes, which are later classified using a taxonomy presented by Fluri and Gall in \cite{classifying}. A summary of change types is given in Table \ref{tab:fine_grained_changes}.

\addtocounter{footnote}{1}
\footnotetext{``The significance level of a change is defined as the impact of the change on other source code entities, i.e., how likely is it that other source code entities have to be changed, when a certain change is applied." \cite{classifying}}
\addtocounter{footnote}{-1}

\LTXtable{\textwidth}{tables/fine_grained_changes.tex}

\begin{diff}{Condition expression change}
\begin{lstdiff}{2,10}
static int abs(int x) {
	if (x < 0) {
		return x;
	} else {
		return -x;
	}
}
|\vfill\columnbreak|
static int abs(int x) {
	if (x >= 0) {
		return x;
	} else {
		return -x;
	}
}
\end{lstdiff}
\end{diff}

\subsection{Feature engineering}
\label{sec:feature_engineering}
Basing on fined-grained code changes (extracted by the algorithm described in previous section) as well as commit meta-information, each instance of my model is ascribed a number of features. As instances represent groups of commits, most features are computed by an aggregation of parameters of single commits (\textit{e.g. via} averaging or summing). Some attributes, related to commits' authors, utilize not only local, but also global information. They require an evaluation of the authors' experience, which cannot be computed using only data related to a single method. All the features are presented in Table \ref{tab:features}.

\LTXtable{\textwidth}{tables/features.tex}

\section{Bug-proneness estimation}
\label{sec:bug-proneness}
For the sake of simplicity, I decided to use a plain textual match to identify the bug-fixing commits. Log messages are searched for one of the following strings: \texttt{"bug"}, \texttt{"fix"}, or \texttt{"issue"} (the matching is case-insensitive). The problem of identification of bug-fixes has been described in more detail in Section \ref{sec:bug-fixes}.

After identifying the bug-fixes, defect-proneness has to be estimated for the other commits. As the assumptions put forward in Section \ref{sec:general_assumptions} do not impose any particular method of estimation, I propose three different strategies which could be used fo this purpose:
\begin{enumerate}[label=(S\arabic*)]
	\item \textbf{Linear} -- It is the simplest method, which assigns linearly ascending scores to successive commits. The common difference of this sequence is determined by the number of commits in a chunk ($n$).
	\[ \underset{1 \leqslant i \leqslant n}{\forall} lin(i) = \frac{i}{n} \]
	\item \textbf{Geometric} -- This strategy uses geometric progression of bug-proneness. As it is impossible for this kind of sequence to advance so ``smoothly" from 0.0 to 1.0 as the arithmetic sequence, the common ratio needs to be specified by an additional parameter $\alpha$.
	\[ \underset{1 \leqslant i \leqslant n}{\forall} geom(i) = \alpha^{n - i} \]
	\item \textbf{Weighted} -- This method, based on sizes of commits (expressed in numbers of fine-grained changes), assigns higher bug-proneness increases to big commits and lower increases to smaller ones.
	\[ \underset{1 \leqslant i \leqslant n}{\forall} wght(i) =  \underset{1 \leqslant j \leqslant i}{\sum} size(j) \Biggm/ \underset{1 \leqslant j \leqslant n}{\sum} size(j) 	 \]
\end{enumerate}

Table \ref{tab:bug_proneness_strategies} and corresponding Figure \ref{fig:bug_proneness_strategies} present examples of using all of the above strategies.

\LTXtable{\textwidth}{tables/bug_proneness_strategies.tex}

\begin{figure}[h]
\centering
\input{figures/bug_proneness_strategies.tex}
\caption{Bug-proneness estimation strategies}
\label{fig:bug_proneness_strategies}
\end{figure}

\section{Machine learning}
\label{sec:machine_learning}
The final step of the development of a defect prediction tool is training a classifier (which in our case should be called a \emph{regressor} for the continuous character of the bug-proneness parameter) that later would be used for the classification of new instances. My approach incorporates four machine learning algorithms, described in the following sections (and evaluated and compared in chapter~\ref{cha:experiments}).

\subsection{Decision tree}
\label{sec:decision_tree}
According to \cite[p. 263]{encyclopedia}, decision tree is a simple, tree-structured classification model. Its inner nodes represent tests, edges correspond to possible test outputs, while leaves contain potential values of the target variable. For each test, the outcomes are both exhaustive and mutually exclusive. Classification of a new case proceeds in a top-down manner. Starting from the tree root, the classified instance is checked against successive conditions (progressing according to the results) until a leaf is reached, from which the result value is retrieved.

The learning algorithm for decision trees, called \emph{recursive splitting} (described in \cite[p.~264]{encyclopedia}) also takes a top-down approach. Beginning with a set of training instances, the best possible split of this set is chosen. Candidates for this choice are all values of attributes appearing in the training dataset. For a discrete attribute, the split has a trivial form -- one subset corresponding to one value. For a numerical one, an inequality test or some kind of discretization is required. Quality of the possible splits (or, more precisely, its reciprocal called \emph{impurity}) is evaluated with measures such as \emph{information entropy} \cite{Shannon} or \emph{Gini index} \cite{Gini}. After choosing the best parameter, the training set is appropriately divided and the algorithm proceeds recursively for each subset. The procedure terminates when the processed subset can no longer be split (either because all instances belong to one class, or due to their inseparability), or when the predefined maximum tree depth is reached.

\begin{figure}[h]
\centering
\input{figures/decision_tree.tex}
\caption[Decision tree example]{Decision tree example\footnotemark}
\label{fig:decision_tree}
\end{figure}
\footnotetext{This tree describes the \emph{Play Golf} data set. ``P" labels indicate weather suitable for playing golf, while ``N" means unsuitable weather. This figure was copied from \cite{decision_trees}.}

\subsection{Random forest}
\label{sec:random_forest}

Random forest, introduced by Breiman in \cite{random_forests}, is an \emph{ensemble learning algorithm}, using the decision tree (see Section \ref{sec:decision_tree}) as the base classifier. It combines two important techniques: \emph{bagging} and \emph{random subspace method}.

Basically, a random forest is a set of decision trees. Each one of them is trained using a separate random sample of an input training set. The output of such ensemble for test instances is obtained from individual responses by computing their mode (in case of classification) or average (regression). This method, called \emph{bagging} or \emph{bootstrap aggregating}, reduces the instability of the base classifier and improves its accuracy \cite{bagging}.

\emph{Random subspace method} increases the diversity between members of the ensemble by choosing random subspaces of the features space to be used \cite[p.~828]{encyclopedia}. In case of random forests, this means that each choice of a data split is restricted to a subset\footnote{Typically of size $\sqrt{n}$ where $n$ is the total number of features.} of features (not all possibilities are taken into account) \cite{random_forests}.
 
\subsection{Support vector machine}
\label{sec:svm}
Support vector machine is a classification model based on the construction of a hyperplane or a set of hyperplanes dividing the feature space of the data. The original form of the algorithm, proposed by Vapnik and Lerner in \cite{svm}, assumes linear separability of classes. The hyperplane is constructed in a way which minimizes the distance between the hyperplane and the closest points from both classes.

Figure \ref{fig:svm} shows two examples of separation of the same set of instances in a two-dimensional space. While both lines separate the classes properly, only \textbf{A} is optimal in the aforementioned sense. The \emph{support vectors} (marked with larger points) are those which are placed closest to the separation line. By a formalization of the maximum-margin hyperplane problem and its transformation to the dual form, one can show that the result classifier relies only on these vectors \cite[pp. 942--944]{encyclopedia}.

\begin{figure}[h]
\centering
\input{figures/svm.tex}
\caption{SVM example}
\label{fig:svm}
\end{figure}

The original version of SVM is a linear classifier, which is a significant disadvantage when dealing with non-linearly separable classes. This issue was addressed by Boser \emph{et al.} in \cite{Boser} through the incorporation of \emph{kernel functions} to the model. This technique allows for a transformation of feature vectors into a high-dimensional space. However, by using the \emph{kernel trick}, no explicit representation of the transformed vectors is created. Another improvement to the algorithm, which enables SVM to handle mislabeled cases, is the \emph{soft margin} introduced by Cortes and Vapnik in \cite{Cortes}.

\subsection{Neural network}
\label{sec:neural_net}

(Artificial) neural network is a learning algorithm based on the structure of neural connections in the human brain. A neural network is composed of simple nodes, known as \emph{neurons}, connected by edges. A neuron is a ``nonlinear, parameterized, bounded function" \cite[p. 2]{neural_networks}. It has several inputs (variables) and exactly one output (value). Typical neuron can be described with the following equation:
\[y(x) = f\left(w_0 + \sum\limits_{i = 1}^{n} (x_i \cdot w_i) \right)\]
where $x_i$ are variables, $w_i$ are appropriate weights, $w_0$ is an additional constant called \emph{bias}, and $f$ is an \emph{activation function} (usually a $sigmoid$ function).

A neural net consists of several layers of neurons. Attribute values of classified instances are fed to the first layer. Then, successively, outputs of lower layers are propagated along the edges to the higher ones, up to the classifier output. Learning of a network is performed via adjustment of weights associated with the edges by the \emph{backpropagation} algorithm \cite[p. 31]{neural_networks}.

\chapter{Implementation}
\label{cha:implementation}
The defect prediction method described in Chapter \ref{cha:approach} has been implemented in form of an integrated tool -- \textsc{ChangeAnalyzer}. It is an open source Java software, licensed under Apache License 2.0 \cite{apache}. The code repository can be found on \textsc{GitHub}: \url{https://github.com/Wiezzel/changeanalyzer}. The following sections describe the languages and tools used for the development (Section \ref{sec:languagess}), interface (Section \ref{sec:interface}), and architecture (Section \ref{sec:architecture}) of the program.

\section{Languages and tools}
\label{sec:languagess}

\subsection{Java} 

Java \cite{java} is an object-oriented programming language, developed by James Gosling at \textsc{Sun Microsystems}, currently supported by \textsc{Oracle Corporation}. It has been chosen as the implementation language of \textsc{ChangeAnalyzer} due to its portability, wide availability of useful libraries (described further), and author's experience in Java programming.

\subsubsection*{JGit}
\textsc{JGit} \cite{jgit} is an implementation of \textsc{Git} \cite{git} version control system in Java. It allows for programmatically performing such operations as inspecting a repository, tracking commits affecting a given file, or retrieving particular file revisions. \textsc{JGit} is used at the very beginning of the feature extraction process to obtain consecutive revisions of a source file, which are later compared by \textsc{ChangeDistiller}.

\subsubsection*{ChangeDistiller}
The \textsc{ChangeDistiller} \cite{change_distilling} is an \textsc{Eclipse} plugin (also available as a stand-alone library), developed in order to extract fine-grained source code changes by syntax-aware file differentiation (for details see Section \ref{sec:change_distilling}). \textsc{ChangeDistiller} is incorporated in \textsc{ChangeAnalyzer} to compare abstract syntax trees of successive file versions provided by \textsc{JGit} and compute the numbers of changes of particular types (see Table \ref{tab:fine_grained_changes}).

\subsubsection*{Weka}
The \textsc{Waikato Environment for Knowledge Analysis} (WEKA) \cite{weka} is an open source data mining software suite written in Java. It contains a broad collection of state-of-the-art machine learning algorithms and can be used either as a stand-alone application, or as a library. \textsc{ChangeAnalyzer} utilizes WEKA to train various predictive models (described in Section \ref{sec:machine_learning}) and predict the defect-proneness of unclassified methods.

\subsection{R}

R \cite{r} ...

\subsubsection*{foreign}
\texttt{foreign} \cite{foreign} ...

\subsubsection*{cvTools}
\texttt{cvTools} \cite{cvTools} ...

\subsubsection*{randomForest}
\texttt{randomForest} \cite{randomForest} ...

\subsubsection*{rpart}
\texttt{rpart} \cite{rpart} ...

\subsubsection*{e1071}
\texttt{e1071} \cite{e1071} ...

\subsubsection*{nnet}
\texttt{nnet} \cite{nnet} ...

\section{Interface}
\label{sec:interface}
[\textit{In this section I will describe functions provided by the implemented tool - ChangeAnalyzer.}]

\section{Architecture}
\label{sec:architecture}
[\textit{In this section I will present the architecture of ChangeAnalyzer.}]

\chapter{Experiments}
\label{cha:experiments}
[\textit{This chapter will contain information about performed experiments which use ChangeAnalyzer and serve as a basis for the evaluation of the proposed approach.}]

\section{Methodology}
\label{sec:methodology}
[\textit{In this section I will describe the experimental methods used.}]

\section{Experimental set-up}
\label{sec:set-up}
[\textit{In this section I will describe the configuration of ChangeAnalyzer used for performing experiments described in the next section.}]

\section{Results}
\label{sec:results}
[\textit{This section will present the results of the experiments in form of tables and charts.}]

\chapter{Conclusions}
\label{cha:conclusions}
[\textit{This chapter will contain the final conclusions of my thesis.}]

\section{Commentary of experimental results}
\label{sec:commentary}
[\textit{In this section I will comment the experimental results.}]

\section{Threats to validity}
\label{sec:threats}
[\textit{In this section I will describe possible threats to the validity of my experiments.}]

\section{Assessment of the approach}
\label{sec:assessment}
[\textit{In this section I will extend the commentary presented in the previous section into a more general judgement of the proposed approach.}]

\section{Possible further work}
\label{sec:further_work}
[\textit{In this section I will outline the possibilities of continuing my work.}]

\printbibliography[heading=bibintoc]

\end{document}